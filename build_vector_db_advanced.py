# build_vector_db_advanced.py (æ–¹æ¡ˆ A)
import os
import re
from typing import List, Dict
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter
# ========================
# é…ç½®åŒºï¼ˆæŒ‰éœ€ä¿®æ”¹è·¯å¾„ï¼‰
# ========================
PDF_DIR = "E:/code/studentRAG/data"          # å­˜æ”¾å­¦ç”Ÿæ‰‹å†Œ PDF çš„æ–‡ä»¶å¤¹
CHROMA_PATH = "E:/code/studentRAG/chroma_db_advanced"  # å‘é‡åº“å­˜å‚¨è·¯å¾„
MAX_BATCH_SIZE = 5000
# ========================
# 1. ç»“æ„åŒ–æ–‡æ¡£åŠ è½½ä¸é¢„å¤„ç†
# ========================
def load_and_split_pdfs_advanced(pdf_dir: str) -> List[Document]:
    """
    ä½¿ç”¨ LangChain åŠ è½½ PDF å¹¶è¿›è¡Œç»“æ„åŒ–å’Œè¯­ä¹‰åˆ†å—
    """
    documents = []
    
    # åŒºåˆ†æ‰‹å†Œç±»å‹å¹¶è®¾ç½®å¯¹åº”çš„å…ƒæ•°æ®
    # ä½ å¯ä»¥æ ¹æ®æ–‡ä»¶åè§„åˆ™è¿›ä¸€æ­¥ç»†åŒ–ï¼Œä¾‹å¦‚ 'undergrad' å’Œ 'grad'
    
    for filename in os.listdir(pdf_dir):
        if not filename.lower().endswith(".pdf"):
            continue
        
        filepath = os.path.join(pdf_dir, filename)
        print(f"ğŸ“„ æ­£åœ¨è§£æ: {filename}")
        
        # 1. ä½¿ç”¨ UnstructuredLoader è¿›è¡Œç»“æ„åŒ–è§£æ
        # mode="elements" å¯ä»¥å°† PDF å†…å®¹åˆ†å‰²æˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ç­‰å…ƒç´ 
        loader = UnstructuredFileLoader(
            filepath, 
            mode="elements", 
            strategy="fast",# å°è¯•ä½¿ç”¨å¿«é€Ÿç­–ç•¥
            languages=["zh"]
        )
        
        # åŠ è½½æ–‡æ¡£å…ƒç´ 
        elements = loader.load()
        
        # 2. é€’å½’å­—ç¬¦åˆ†å‰²ï¼ˆä¿ç•™æ®µè½/æ ‡é¢˜å®Œæ•´æ€§ï¼‰
        # å°è¯•ä½¿ç”¨ä¸åŒçš„åˆ†éš”ç¬¦ï¼Œä¼˜å…ˆæŒ‰è‡ªç„¶æ®µè½å’Œå¥å­åˆ†å‰²
        recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=900,
            chunk_overlap=200,
            separators=["\n\n\n", "\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )

        # 3. è¿›è¡Œåˆ†å—
        chunks = recursive_splitter.split_documents(elements)
        
        # 4. ä¼˜åŒ–ï¼šæ·»åŠ å…ƒæ•°æ®
        for i, chunk in enumerate(chunks):
            # å°†æºæ–‡ä»¶åä½œä¸ºä¸»è¦å…ƒæ•°æ®
            chunk.metadata['source'] = filename
            
            # è‡ªåŠ¨æ·»åŠ æ‰‹å†Œç±»å‹å…ƒæ•°æ® (ç”¨äº RAG æ—¶çš„å…ƒæ•°æ®è¿‡æ»¤)
            if "æœ¬ç§‘ç”Ÿ" in filename or "æœ¬ç§‘" in filename:
                chunk.metadata['handbook_type'] = "æœ¬ç§‘ç”Ÿ"
            elif "ç ”ç©¶ç”Ÿ" in filename or "ç¡•å£«" in filename or "åšå£«" in filename:
                chunk.metadata['handbook_type'] = "ç ”ç©¶ç”Ÿ"
            else:
                 chunk.metadata['handbook_type'] = "é€šç”¨"
            
            # ç¡®ä¿ id æ˜¯å”¯ä¸€çš„
            chunk.metadata['id'] = f"{filename}_{i}"
            
            # é‡å‘½å content
            chunk.page_content = chunk.page_content.strip()

        print(f"  ğŸ“¦ åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å— (å«å…ƒæ•°æ®: handbook_type)")
        documents.extend(chunks)
    
    return documents

# ========================
# 2. æ„å»º Chroma å‘é‡åº“
# ========================
def build_chroma_db_advanced():
    # æ›¿æ¢åŸå§‹çš„ load_and_split_pdfs å‡½æ•°
    docs = load_and_split_pdfs_advanced(PDF_DIR)
    
    if not docs:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼Œé€€å‡ºã€‚")
        return
    
    print(f"\nğŸ” å¼€å§‹æ„å»ºå‘é‡åº“ï¼ˆä½¿ç”¨ BGE-large-zh-v1.5ï¼‰...")
    
    # åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯ï¼ˆæŒä¹…åŒ–åˆ°ç£ç›˜ï¼‰
    client = PersistentClient(path=CHROMA_PATH)
    
    # ä½¿ç”¨ BGE ä¸­æ–‡ embedding æ¨¡å‹
    embedding_func = SentenceTransformerEmbeddingFunction(
        model_name="BAAI/bge-large-zh-v1.5",
        device="cuda" if _is_cuda_available() else "cpu"
    )
    
    # åˆ›å»º collection
    collection = client.get_or_create_collection(
        name="student_handbook_advanced",
        embedding_function=embedding_func,
        metadata={"hnsw:space": "cosine"}
    )
    
    # æå– Chroma éœ€è¦çš„æ ¼å¼
    ids = [d.metadata['id'] for d in docs]
    documents_content = [d.page_content for d in docs]
    # æå–å…ƒæ•°æ® (æ³¨æ„ï¼šChroma çš„å…ƒæ•°æ®è¦æ±‚æ˜¯å­—å…¸)
    metadatas = []
    
    # å®šä¹‰éœ€è¦æ’é™¤çš„ LangChain/Unstructured å†…éƒ¨é”®
    EXCLUDE_KEYS = ['id', 'metadata_storage_key', 'type', 'filetype', 'languages', 'last_modified']
    
    for doc in docs:
        clean_metadata = {}
        for k, v in doc.metadata.items():
            # 1. æ’é™¤å†…éƒ¨ä½¿ç”¨çš„å¤æ‚é”®
            if k in EXCLUDE_KEYS:
                continue
            
            # 2. ç¡®ä¿å€¼æ˜¯ç®€å•ç±»å‹ (è™½ç„¶å¤§éƒ¨åˆ†åº”è¯¥åœ¨ä½ è‡ªå®šä¹‰çš„é”®ä¸­ï¼Œä½†ä¿é™©èµ·è§)
            if isinstance(v, (str, int, float, bool)) or v is None:
                clean_metadata[k] = v
            # 3. å¦‚æœéœ€è¦ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¤„ç†æˆ–å¿½ç•¥å…¶ä»–å¤æ‚ç±»å‹
            
        metadatas.append(clean_metadata)
    
    # æ·»åŠ æ–‡æ¡£ï¼ˆè‡ªåŠ¨åµŒå…¥ï¼‰
    total_chunks = len(docs)
    print(f"ğŸš€ æ€»å…± {total_chunks} ä¸ªç‰‡æ®µã€‚å¼€å§‹åˆ†æ‰¹å†™å…¥ (æ‰¹æ¬¡å¤§å°: {MAX_BATCH_SIZE})...")
    
    # --- ã€æ ¸å¿ƒä¿®æ”¹ï¼šå®ç°åˆ†æ‰¹æ¬¡å†™å…¥ã€‘ ---
    for i in range(0, total_chunks, MAX_BATCH_SIZE):
        batch_ids = ids[i:i + MAX_BATCH_SIZE]
        batch_documents = documents_content[i:i + MAX_BATCH_SIZE]
        batch_metadatas = metadatas[i:i + MAX_BATCH_SIZE]
        
        print(f"  â¡ï¸ æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {i//MAX_BATCH_SIZE + 1}: æ·»åŠ  {len(batch_ids)} ä¸ªç‰‡æ®µ...")
        
        try:
            collection.add(
                ids=batch_ids,
                documents=batch_documents,
                metadatas=batch_metadatas
            )
        except ValueError as e:
            print(f"  âŒ æ‰¹æ¬¡ {i//MAX_BATCH_SIZE + 1} æ·»åŠ å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ MAX_BATCH_SIZE æ˜¯å¦ä»ç„¶è¿‡å¤§ï¼Œæˆ–è€…è¿›è¡Œé”™è¯¯å¤„ç†
            break # åœæ­¢åç»­å¤„ç†
    
    print(f"\nâœ… å‘é‡åº“æ„å»ºå®Œæˆï¼å…± {len(docs)} ä¸ªç‰‡æ®µ")
    print(f"ğŸ“ å­˜å‚¨ä½ç½®: {CHROMA_PATH}")
    print(f"ğŸ’¡ å…³é”®æ”¹è¿›ï¼šæ–°å¢ 'handbook_type' å…ƒæ•°æ®ï¼Œå¯ç”¨äº RAG æ—¶çš„ç²¾ç¡®è¿‡æ»¤ã€‚")

# ========================
# è¾…åŠ©å‡½æ•°ï¼šæ£€æµ‹ CUDA
# ========================
def _is_cuda_available():
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False

# ========================
# ä¸»ç¨‹åºå…¥å£
# ========================
if __name__ == "__main__":
    build_chroma_db_advanced()