# interactive_assistant.py - ä½¿ç”¨ EdgeFn äº‘ç«¯å¤§æ¨¡å‹çš„ RAG åŠ©æ‰‹
import os
import requests
import json
from typing import List, Optional
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# ========================
# é…ç½®åŒº
# ========================
CHROMA_PATH = "E:/code/studentRAG/chroma_db_advanced"  # å‘é‡åº“å­˜å‚¨è·¯å¾„
EMBEDDING_MODEL = "BAAI/bge-large-zh-v1.5"
TOP_K = 5  # å¬å›ç‰‡æ®µæ•°é‡

# --- äº‘ç«¯å¤§æ¨¡å‹é…ç½® ---
CLOUD_API_BASE = "https://api.edgefn.net/v1"
CLOUD_MODEL_NAME = "Qwen3-Next-80B-A3B-Instruct"
CLOUD_API_KEY = os.getenv("BAISAN_API")  # ä»ç³»ç»Ÿç¯å¢ƒå˜é‡ BAISAN_API è¯»å–

if not CLOUD_API_KEY:
    raise EnvironmentError(
        "âŒ ç¯å¢ƒå˜é‡ 'BAISAN_API' æœªè®¾ç½®ï¼\n"
        "è¯·åœ¨è¿è¡Œå‰è®¾ç½® API å¯†é’¥ï¼Œä¾‹å¦‚ï¼ˆPowerShellï¼‰ï¼š\n"
        "$env:BAISAN_API='your_actual_api_key_here'\n"
        "æˆ–ï¼ˆCMDï¼‰ï¼š\n"
        "set BAISAN_API=your_actual_api_key_here"
    )

# ========================
# Prompt Template
# ========================
RAG_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šã€ä¸¥è°¨çš„å¤§å­¦å­¦ç”Ÿæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„**ä¸Šä¸‹æ–‡èµ„æ–™**ï¼Œå‡†ç¡®ã€ç®€æ´åœ°å›ç­”å­¦ç”Ÿçš„æé—®ã€‚

è¯·éµå®ˆä»¥ä¸‹ä¸¥æ ¼çš„çº¦æŸå’Œæ­¥éª¤ï¼š
1.  **ä¸¥æ ¼åŸºäºä¸Šä¸‹æ–‡ï¼š** ä½ çš„å›ç­”å¿…é¡»ä¸”ä»…åŸºäºã€ä¸Šä¸‹æ–‡èµ„æ–™ã€‘ä¸­æä¾›çš„ä¿¡æ¯ã€‚
2.  **æå–å¹¶å¼•ç”¨ï¼š** åœ¨å›ç­”ä¸­ï¼Œç”¨ä¸­æ–‡åˆ†ç‚¹åˆ—å‡ºå…³é”®ä¿¡æ¯ã€‚
3.  **æ³¨æ˜æ¥æºï¼š** åœ¨å›ç­”çš„æœ€åï¼ŒåŠ¡å¿…ä»¥â€œèµ„æ–™æ¥æºï¼š[æ–‡ä»¶å]â€çš„æ ¼å¼ï¼Œæ³¨æ˜ä½ ä½¿ç”¨çš„æ‰€æœ‰ä¸Šä¸‹æ–‡èµ„æ–™çš„æ–‡ä»¶åï¼ˆæ¥è‡ªä¸Šä¸‹æ–‡ä¸­çš„ 'source'ï¼‰ã€‚
4.  **æ— æ³•å›ç­”æ—¶ï¼š** å¦‚æœä¸Šä¸‹æ–‡èµ„æ–™æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·ç¤¼è²Œåœ°å›ç­”ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»ç°æœ‰çš„å­¦ç”Ÿæ‰‹å†Œèµ„æ–™ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚â€

---
ã€ä¸Šä¸‹æ–‡èµ„æ–™ã€‘
{context}
---
ã€å­¦ç”Ÿé—®é¢˜ã€‘
{question}
"""

# ========================
# åˆå§‹åŒ–å‡½æ•°
# ========================
def initialize_rag_system():
    """åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯å’Œ Embedding Function"""
    print(f"ğŸ”„ åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
    
    client = PersistentClient(path=CHROMA_PATH)
    
    embedding_func = SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL,
        device="cuda" if _is_cuda_available() else "cpu"
    )
    
    collection = client.get_collection(
        name="student_handbook_advanced",
        embedding_function=embedding_func
    )
    
    print(f"âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚å½“å‰å‘é‡åº“ç‰‡æ®µæ•°: {collection.count()}")
    return collection

def _is_cuda_available():
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False

# ========================
# äº‘ç«¯ API è°ƒç”¨å‡½æ•°ï¼ˆæ ¸å¿ƒï¼‰
# ========================
def generate_response_cloud_api(prompt: str) -> str:
    """è°ƒç”¨ EdgeFn äº‘å¹³å°çš„ OpenAI å…¼å®¹ Chat Completions API"""
    url = f"{CLOUD_API_BASE}/chat/completions"
    
    headers = {
        "Authorization": f"Bearer {CLOUD_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": CLOUD_MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å¤§å­¦å­¦ç”Ÿäº‹åŠ¡åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„èµ„æ–™å›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1,
        "max_tokens": 1024,
        "stream": False
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=60)
        response.raise_for_status()
        
        data = response.json()
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
        return content if content else "âš ï¸ äº‘ç«¯æ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ã€‚"
        
    except requests.exceptions.Timeout:
        return "âŒ è¯·æ±‚è¶…æ—¶ï¼šäº‘ç«¯ API å“åº”è¿‡æ…¢ï¼Œè¯·ç¨åå†è¯•ã€‚"
    except requests.exceptions.HTTPError as e:
        try:
            error_msg = response.json().get("error", {}).get("message", str(e))
        except:
            error_msg = response.text[:200]
        return f"âŒ HTTP é”™è¯¯ {response.status_code}: {error_msg}"
    except requests.exceptions.RequestException as e:
        return f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{e}"
    except json.JSONDecodeError:
        return f"âŒ API è¿”å›é JSON æ ¼å¼ï¼š{response.text[:200]}"

# ========================
# RAG æŸ¥è¯¢ä¸»å‡½æ•°
# ========================
def rag_query(collection, query: str, handbook_type: Optional[str] = None):
    """æ‰§è¡Œ RAG æŸ¥è¯¢ï¼Œæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤"""
    where_filter = None
    if handbook_type:
        where_filter = {"handbook_type": {"$eq": handbook_type}}
        print(f"ğŸ” ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤: ä»…æœç´¢ã€{handbook_type}ã€‘æ‰‹å†Œã€‚")

    try:
        results = collection.query(
            query_texts=[query],
            n_results=TOP_K,
            where=where_filter,
            include=['documents', 'metadatas']
        )
    except Exception as e:
        return f"âŒ å‘é‡åº“æŸ¥è¯¢å¤±è´¥ï¼š{e}"

    context_list = results.get('documents', [[]])[0]
    metadata_list = results.get('metadatas', [[]])[0]
    
    if not context_list:
        return f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»ç°æœ‰çš„æ‰‹å†Œèµ„æ–™ä¸­æ‰¾åˆ°ä¸é—®é¢˜ â€œ{query}â€ ç›¸å…³çš„ä¿¡æ¯ã€‚"

    context_str = ""
    for i, (doc, meta) in enumerate(zip(context_list, metadata_list)):
        source_name = meta.get('source', 'æœªçŸ¥æ¥æº')
        context_str += f"--- ç‰‡æ®µ {i+1} (æ¥æº: {source_name}) ---\n{doc}\n"

    final_prompt = RAG_PROMPT_TEMPLATE.format(context=context_str, question=query)
    return generate_response_cloud_api(final_prompt)

# ========================
# ä¸»ç¨‹åºå…¥å£
# ========================
if __name__ == "__main__":
    collection = initialize_rag_system()
    
    print("\n" + "="*50)
    print(f"ğŸš€ æ™ºèƒ½å­¦ç”ŸåŠ©æ‰‹å·²å¯åŠ¨ (äº‘ç«¯æ¨¡å‹: {CLOUD_MODEL_NAME})")
    print("æç¤ºï¼šè¾“å…¥ 'æœ¬ç§‘ç”Ÿ' æˆ– 'ç ”ç©¶ç”Ÿ' åˆ‡æ¢æœç´¢èŒƒå›´ï¼Œè¾“å…¥ 'quit' é€€å‡ºã€‚")
    print("="*50)

    current_filter = None
    
    while True:
        user_input = input(f"\n[{current_filter or 'é€šç”¨'}] ä½ æƒ³é—®ï¼š").strip()
        
        if user_input.lower() == 'quit':
            print("ğŸ‘‹ åŠ©æ‰‹å·²å…³é—­ã€‚")
            break
        
        if user_input in ["æœ¬ç§‘ç”Ÿ", "ç ”ç©¶ç”Ÿ"]:
            current_filter = user_input
            print(f"âœ… æœç´¢èŒƒå›´å·²åˆ‡æ¢ä¸ºï¼šã€{current_filter}ã€‘æ‰‹å†Œã€‚")
            continue
        
        if not user_input:
            continue

        print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
        result = rag_query(collection, user_input, current_filter)
        
        print("\n" + "--- æ™ºèƒ½åŠ©æ‰‹å›ç­” ---")
        print(result)
        print("----------------------")